\documentclass[../integ-proba.tex]{subfiles}

\begin{document}

    \chapter{Variables aléatoires}

    Voyons maintenant ce qu'est une variable aléatoire.
    C'est une appellation étrange, puisqu'une variable aléatoire n'est en fait ni une variable, ni quelque-chose d'aléatoire.
    En réalité, il s'agit simplement d'une fonction qui possède certaines propriétés (notamment la mesurabilité).
    En général, nous nous limiterons ici à l'étude des variables aléatoire réelles, des vecteurs aléatoires et des variables aléatoires discrètes.

    \section{Définitions}

    Dans toute cette section, on se donne un espace probabilisé $\left(\Omega, \A, \Pb\right)$.
    On rappelle que $\B(E)$ désigne la tribu des boréliens de l'espace topologique $E$.

    \begin{defi}
        Une \textbf{variable aléatoire réelle} est une fonction mesurable
        \begin{displaymath}
            X:\left(\Omega, \A, \Pb\right) \longrightarrow \left(\R, \B(\R)\right)
        \end{displaymath}
    \end{defi}

    \begin{defi}
        Un \textbf{vecteur aléatoire} est une fonction mesurable
        \begin{displaymath}
          X:\left(\Omega, \A, \Pb\right) \longrightarrow \left(\R^n, \B(\R^n)\right)
        \end{displaymath}
    \end{defi}

    \begin{defi}
        \label{defi:variables_aleatoires_discretes}
        Une \textbf{variable aléatoire discrète} est une fonction mesurable
        \begin{displaymath}
          X:\left(\Omega, \A, \Pb\right) \longrightarrow \left(\N, \mathcal{P}(\N)\right)
        \end{displaymath}
    \end{defi}

    On peut également introduire la définition suivante, qui ne sera pas utile en pratique, mais qui a le mérite d'unifier les trois définitions précédentes :

    \begin{defi}
        Plus généralement, si $E$ est un espace topologique quelconque, une \textbf{variable aléatoire} est une fonction mesurable
        \begin{displaymath}
          X:\left(\Omega, \A, \Pb\right) \longrightarrow \left(E, \B(E)\right)
        \end{displaymath}
    \end{defi}

    \begin{rem}
        On remarquera que, exceptés l'espace d'arrivée, ces définitions sont identiques : dans tous les cas, on choisit des fonctions mesurables, et conformément à la simplification expliquée à la remarque~\ref{rem:simplification_notation_fonctions_mesurables}, on choisit la tribu borélienne sur l'espace d'arrivée.
        Cela peut ne pas sembler évident pour la définition~\ref{defi:variables_aleatoires_discretes} concernant les variables aléatoires discrètes : on rappelle donc que les boréliens de $\N$ (muni de la topologie discrète) sont exactement les parties de $\N$ (pour plus de détails, se référer à l'exemple~\ref{exemple:boreliens_et_topologie_discrete}).
    \end{rem}

    \begin{rem}
        On fera attention au fait que :
        \begin{itemize}
            \itemb une variable aléatoire est une fonction qui prend ses valeurs dans $\Omega$ ;
            \itemb une mesure de probabilité est une fonction qui prend ses valeurs dans $\A$.
        \end{itemize}
        Ainsi, si $\omega \in \Omega$ :
        \begin{itemize}
            \itemb écrire $X(\omega)$ a du sens.
            \itemb écrire $\Pb(\omega)$ n'a pas de sens ($\omega$ n'a aucune chance d'être dans $\A$, puisque ce n'est même pas une partie de $\Omega$).
            \itemb écrire $\Pb(\{\omega\})$ a du sens dès que $\{\omega\}\in \A$ (\textit{ie} dès que $\{\omega\}$ est mesurable).
            \itemb écrire $X(\{\omega\})$ n'a pas de sens.
        \end{itemize}
    \end{rem}

    \begin{prop}
        \label{prop:composition_var_bor}
        Soit $E$ et $F$ deux espaces topologiques.
        Soit $X:\Omega \longrightarrow E$ une variable aléatoire à valeurs dans $E$.
        Soit $g:E \longrightarrow F$ une fonction borélienne.
        Alors $g \circ X$ est une variable aléatoire à valeurs dans $F$.
    \end{prop}

    \begin{rem}
        Cette proposition est une conséquence immédiate du résultat de composition de fonctions mesurables.
    \end{rem}

    \section{Loi d'une variable aléatoire}

    Dans toute cette section, on se donne
    \begin{itemize}
        \itemb un espace probabilisé $\left(\Omega, \A, \Pb\right)$ ;
        \itemb une variable aléatoire quelconque $X$ à valeurs dans un espace topologique $\left(E, \B(E)\right)$.
    \end{itemize}

    Ces définitions peuvent être données pour des variables aléatoires quelconques, mais seront utiles en pratique pour les trois types de variables aléatoires que nous avons vues.

    \begin{defi}
        La \textbf{loi de probabilité de} $X$ $\Pb_X$ est définie comme :
        \begin{displaymath}
            \Pb_X=\Pb \circ X^{-1}
        \end{displaymath}
        C'est la mesure image de $\Pb$ par $X$.
        Il s'agit d'une loi de probabilité sur $\left(E, \B(E) \right)$
    \end{defi}
    % todo parler de mesure image quelque part d'autre

    \begin{rem}
        Ces notations étant lourdes, on préfèrera en introduire de nouvelles, plus simples à interpréter.
        Attention : elles n'ont pas de sens en elles-mêmes, mais ont l'avantage de bien faire comprendre la notion sous-jacente.
        \begin{itemize}
            \itemb Si $F \in \B(E)$, on notera :
            \begin{displaymath}
                \begin{array}{rcl}
                    \Pb(X \in F) := \Pb \circ X^{-1}(F) = \Pb(X^{-1}(F)) = \Pb_X(F)
                \end{array}
            \end{displaymath}
            \itemb Si $F = \left\{ e \right\}$, alors notera plutôt :
            \begin{displaymath}
                \begin{array}{rcl}
                    \Pb(X = e) := \Pb \circ X^{-1}(\left\{ e \right\}) = \Pb(X^{-1}(\left\{ e \right\})) = \Pb_X(\left\{ e \right\})
                \end{array}
            \end{displaymath}
            \itemb Si $E=\R$, et que $F = [x,y]$, alors on notera plutôt :
            \begin{displaymath}
                \begin{array}{rcl}
                    \Pb(x \leq X \leq y) := \Pb \circ X^{-1}([x,y]) = \Pb(X^{-1}([x,y])) = \Pb_X([x,y])
                \end{array}
            \end{displaymath}
            \itemb etc\ldots
        \end{itemize}
    \end{rem}

    \begin{defi}
        Soit $\Q$ une loi de probabilité sur $\left(E, \B(E) \right)$.
        On dit que $X$ \textbf{suit la loi} $\Q$ lorsque $\Pb_X = \Q$.
    \end{defi}

    \begin{rem}
        Conformément à la remarque~\ref{rem:densite}, et en utilisant le théorème~\ref{thm:cns_densite}, \textbf{si $\Q$ est une loi sur $\R$ qui admet une densité $f$, pour prouver que $X$ suit la loi $\Q$, il suffit de vérifier que $X$ est bien à valeurs dans $\R$, puis de montrer que $\forall x \in \R, \Pb(X \leq x) = \int_{-\infty}^x f(t) \textnormal{d}t$.}
        En effet,
        \begin{displaymath}
            \begin{array}{rcl}
                \Pb(X \leq x) & = & \Pb(X \in ]-\infty, x]) \\
                           & = & \Pb(X^{-1}(]-\infty, x])) \\
                           & = & \Pb_X(]-\infty, x]) \\
                           & = & \int_{-\infty}^x f(t) \textnormal{d}t
            \end{array}
        \end{displaymath}
        Evidemment, cette condition est aussi une condition nécessaire.

        Dans ce cas, \textbf{on dit que $X$ est une variable aléatoire réelle à densité} (\textit{stricto sensu}, on devrait dire que la loi de $X$ est à densité, mais cet abus de language est toléré).
    \end{rem}

    \begin{rem}
        Conformément à la remarque~\ref{rem:discretes}, \textbf{si $\Q$ est une loi discrète sur $E$, pour prouver que $X$ suit la loi $\Q$, il suffit de vérifier que $X$ est bien à valeurs dans $E$ , puis de montrer que $\forall e \in E, \Pb(X = e) = \Q(\left\{ e \right\})$.}
        En effet,
        \begin{displaymath}
            \begin{array}{rcl}
                \Pb(X = e) & = & \Pb(X \in \left\{ e \right\}) \\
                           & = & \Pb(X^{-1}(\left\{ e \right\})) \\
                           & = & \Pb_X(\left\{ e \right\})
            \end{array}
        \end{displaymath}
        Evidemment, cette condition est aussi une condition nécessaire.
    \end{rem}

    \begin{rem}
        Attention, ce n'est pas parce que deux variables aléatoires ont la même loi qu'elles sont égales !
        Voici un contre-exemple.
        Il utilise cependant des notions qui n'ont pas encore été abordées à ce stade.

        Considérons l'espace probabilisé $\Omega := \{\text{beau temps}, \text{mauvais temps}\}$ muni de l'ensemble de ses parties, et de la mesure de probabilité $\Pb$ uniforme.
        On a donc :
        \begin{displaymath}
            \Pb(\{\text{beau temps}\}) = \Pb(\{\text{mauvais temps}\}) = \frac{1}{2}
        \end{displaymath}
        Puis posons la variable aléatoire discrète $X$ à valeurs dans $\{0,1\}$, telle que $X(\{\text{beau temps}\})=1$ et $X(\{\text{mauvais temps}\})=0$.

        $X$ suit alors la loi uniforme sur l'espace $\{0,1\}$.

        Considérons maintenant l'espace probabilisé $\Omega' := \{\text{temps chaud}, \text{temps froid}\}$ muni de l'ensemble de ses parties, et de la mesure de probabilité $\Pb'$ uniforme.
        On a donc :
        \begin{displaymath}
            \Pb(\{\text{temps chaud}\}) = \Pb(\{\text{temps froid}\}) = \frac{1}{2}
        \end{displaymath}
        Puis posons la variable aléatoire discrète $Y$ à valeurs dans $\{0,1\}$, telle que $Y(\{\text{temps chaud}\})=1$ et $Y(\{\text{temps froid}\})=0$.

        $Y$ suit également la loi uniforme sur l'espace $\{0,1\}$.

        Nous avons donc explicité deux variables aléatoires qui ont même loi, mais qui ne sont pas égales (elles n'ont pas le même espace de départ).
    \end{rem}

    \section{Moments d'une variable aléatoire réelle ou discrète}

    Dans toute cette section, on se donne
    \begin{itemize}
        \itemb un espace probabilisé $\left(\Omega, \A, \Pb\right)$ ;
        \itemb une variable aléatoire réelle ou discrète.
    \end{itemize}

    On ne pourra pas donner de sens à ces notions dans le cas général d'une variable aléatoire quelconque.
    Nous verrons néanmoins par la suite comment généraliser ces notions aux vecteurs aléatoires.

    \begin{defi}
        Soit $r\in\N$.
        On dit que $X$ \textbf{admet un moment d'ordre} $r$ lorsque $\omega \mapsto X^r(\omega)$ est $\Pb$-intégrable.
    \end{defi}

    \begin{rem}
        On rappelle que l'intégrale de Lebesgue est absolue.
        Ainsi, cette définition et les assertions suivantes sont équivalentes :
        \begin{itemize}
            \itemb $\omega \mapsto \left|X(\omega)\right|^r$ est $\Pb$-intégrable.
            \itemb $\displaystyle \int_\Omega\left|X(\omega)\right|^r\text{d}\Pb(\omega) < +\infty$
        \end{itemize}
    \end{rem}

    \begin{defi}
        Soit $r\in\N$.
        Lorsque $X$ admet un moment d'ordre $r$, on définit son \textbf{moment d'ordre } $r$ comme
        \begin{displaymath}
          m_r \coloneqq \int_\Omega X^r(\omega)\text{d}\Pb(\omega)
        \end{displaymath}
    \end{defi}

    \begin{rem}
        On vérifie aisément, par la relation mesure-intégrale, que toute variable aléatoire admet un moment d'ordre 0, et que celui-ci vaut toujours 1.
    \end{rem}

    \begin{rem}
        \label{rem:esperances_particulieres}
        Voici deux cas particuliers pour vérifier si $X$ admet un moment d'ordre $r$, et pour l'expression du moment le cas échéant :
        \begin{itemize}
            \itemb \textbf{si $X$ est une variable aléatoire réelle de densité $f$, alors $X$ admet un moment d'ordre $r$ ssi}
            \begin{displaymath}
                \int_\R \left|x\right|^r f(x) \textnormal{d}x < + \infty
            \end{displaymath}
            Si c'est le cas, alors :
            \begin{displaymath}
                \esp(X) = \int_\R x^r f(x) \textnormal{d}x
            \end{displaymath}
            La démonstrtion est immédiate à l'aide du théorème~\ref{thm:avantage_densite}.
            \itemb \textbf{si $X$ est une variable aléatoire discrète, alors $X$ admet un moment d'ordre $r$ ssi}
            \begin{displaymath}
                \sum_{k=0}^{+\infty} \left|k\right|^r \Pb(X=k) < + \infty
            \end{displaymath}
            Si c'est le cas, alors :
            \begin{displaymath}
                \esp(X) = \sum_{k=0}^{+\infty} k^r \Pb(X=k)
            \end{displaymath}
            En effet, dans le cas discret, $\Pb_X = fc$, où $f(k) = \Pb_X(\left\{ k \right\}) = \Pb(X = k)$ (cf.\ théorème~\ref{thm:avantage_discret}).
            Puis le théorème~\ref{thm:avantage_densite} fournit l'équivalence et l'égalité.
            Enfin, le théorème~\ref{thm:lien_series} fournit l'expression sous forme de somme.

            Remarquons que tout a été donné ici dans le cas de $\N$, mais tout s'énonce de même dans les autres cas.
        \end{itemize}
    \end{rem}

    \begin{defi}
        On dit que $X$ \textbf{admet une espérance} lorsqu'elle admet un moment d'ordre 1 (\textit{ie} lorsque $\omega \mapsto X\left(\omega\right)$ est $\Pb$-intégrable).
        Son \textbf{espérance} est alors définie comme son moment d'ordre 1.
        Elle est notée $\esp(X)$.

        On note $\mathcal{L}^1\left(\Omega, \A, \Pb\right)$ l'ensemble des variables aléatoires qui admettent une espérance (\textit{ie} l'ensemble des variables aléatoires $\Pb$-intégrables).
    \end{defi}

    \begin{rem}
        Par abus de notation, on pourra écrire $\esp(X)=+\infty$ lorsque $X$ n'admet pas d'espérance et qu'elle est positive.
        Ainsi, on pourra écrire $\esp(X)$ dès que $X$ est positive, qu'elle admette une espérance ou non.
        Vue la place privilégiée qu'occupent les fonctions positives dans la théorie d'intégration de Lebesgue, cette notation ne surprend pas.

        Attention cependant, si $X$ n'admet pas d'espérance et n'est pas positive, on s'interdira formellement d'écrire $\esp(X)$ (car l'intégrale sous-jacente n'existe peut-être même pas !).
    \end{rem}

    \begin{rem}
        Ces considérations étant faites, on remarque que :
        \begin{itemize}
            \itemb $X$ admet une espérance ssi $\esp(\left| X \right|) < +\infty$ ;
            \itemb $X$ admet un moment d'ordre $r$ ssi $\esp(\left| X \right|^r) < +\infty$ ;
            \itemb si $X$ admet un moment d'ordre $r$, alors $m_r = \esp(X^r)$.
        \end{itemize}
    \end{rem}

    \begin{prop}
        \label{prop:moments_cascade}
        Soit $r\in\N$.
        Si $X$ admet un moment d'ordre $r$, alors elle admet un moment d'ordre $k$ pour tout $k\in[\![0,r]\!]$.
        Autrement dit, si $X^r$ est $\Pb$-intégrable, alors $X^k$ est $\Pb$-intégrable pour tout $k\in[\![0,r]\!]$.
    \end{prop}

    \begin{proof}
        Soit $k\in[\![0,r]\!]$.
        Nous avons $\left|X\right|^k \leq \left|X\right|^r+1$, donc par croissance et linéarité de l'intégrale :
        \begin{displaymath}
            \int_\Omega \left|X(\omega)\right|^k \text{d}\Pb(\omega) \leq \int_\Omega \left|X(\omega)\right|^r \text{d}\Pb(\omega) + \int_\Omega \text{d}\Pb(\omega)
        \end{displaymath}

        Comme $X$ admet un moment d'ordre $r$, le premier terme de la somme et fini.
        Puis, d'après la relation mesure-intégrale, le deuxième terme de la somme vaut 1 ($\Pb$ est une mesure de probabilité).

        Le terme de gauche est donc fini, donc $X$ admet un moment d'ordre $k$.
    \end{proof}

    \begin{rem}
        \label{rem:warning_moments}
        Attention, on remarquera que ceci n'est vrai que parce que $\Pb$ est une mesure de probabilité !
        Dans le cas d'une mesure quelconque (par exemple, la mesure de Lebesgue), cette démonstration ne permet pas de conclure (car $\lambda(\R) = +\infty$)\ldots et heureusement, car ce résultat est faux !

        Voici un contre-exemple :
        \begin{displaymath}
          x \mapsto \mathds{1}_{\R_+^*}(x)\frac{1}{x}
        \end{displaymath}
        n'est pas intégrable sur $\R$, alors que son carré l'est.
    \end{rem}

    \begin{defi}
        On dit que $X$ \textbf{admet une variance} lorsqu'elle admet un moment d'ordre 2 (\textit{ie} lorsque $\omega \mapsto X^2\left(\omega\right)$ est $\Pb$-intégrable).
        Sa \textbf{variance} est alors définie comme :
        \begin{displaymath}
            \var(X):=\esp\left(\left(X - \esp\left(X\right)\right)^2\right)=\esp\left(X^2\right) - \esp\left(X\right)^2
        \end{displaymath}

        On note $\mathcal{L}^2\left(\Omega, \A, \Pb\right)$ l'ensemble des variables aléatoires qui admettent une variance (\textit{ie} l'ensemble des variables aléatoires qui sont de carré $\Pb$-intégrables).
    \end{defi}

    \begin{rem}
        Cette définition impose de faire trois remarques :
        \begin{itemize}
            \itemb le théorème qui fournit l'égalité de ces deux expressions s'appelle \textbf{le théorème de König-Huygens}.
            Sa démonstration est immédiate par linéarité de l'intégrale.
            \itemb d'après la deuxième expression, l'existence de $\var(X)$ est assurée dès que $X$ admet un moment d'ordre 1 et 2.
            Cependant, bien que la définition demande que $X$ admette un moment d'ordre 2, elle ne demande pas qu'elle admette un moment d'ordre 1.
            Nous sommes en fait sauvés par la proposition~\ref{prop:moments_cascade} !
            \itemb d'après la proposition~\ref{prop:moments_cascade}, $\mathcal{L}^2\left(\Omega, \A, \mathbb{P}\right) \subset \mathcal{L}^1\left(\Omega, \A, \mathbb{P}\right)$.
            A nouveau, on rappelle que ceci n'est vrai que parce que $\mathbb{P}$ est une mesure de probabilité (voir la remarque~\ref{rem:warning_moments})
        \end{itemize}
    \end{rem}

    \section{Formule de transfert}

    Dans toute cette section, on se donne
    \begin{itemize}
        \itemb un espace probabilisé $\left(\Omega, \A, \mathbb{P}\right)$ ;
        \itemb un espace topologique quelconque $E$ ;
        \itemb une variable aléatoire quelconque $X:\Omega \longrightarrow E$.
    \end{itemize}

    \begin{thm}[\textbf{Formule de transfert}]
        Soit $g:E \longrightarrow \R$ (resp. $g:E \longrightarrow \N$) une fonction borélienne.
        Par la proposition~\ref{prop:composition_var_bor}, $g \circ X$ est encore une variable aléatoire ;
        c'est d'ailleurs une variable aléatoire réelle (resp. une variable aléatoire discrète).
        On sait donc donner un sens à son espérance.

        La variable aléatoire réelle (resp. discrète) $g \circ X$ sur $\left(\Omega, \A, \mathbb{P}\right)$ admet une espérance (\textit{ie} $\omega \mapsto (g \circ X)(\omega)$ est $\mathbb{P}$-intégrable) ssi la variable aléatoire réelle (resp. discrète) $g$ sur $\left(E, \B(E), \mathbb{P}_X\right)$ admet une espérance (\textit{ie} $x \mapsto g(x)$ est $\mathbb{P}_X$-intégrable).

        Si c'est le cas, alors on a la \textbf{formule de transfert} :
        \begin{displaymath}
            \esp(g \circ X) = \int_E g(x)\textnormal{d}\mathbb{P}_X(x)
        \end{displaymath}
    \end{thm}

    \begin{rem}
        On comprend pourquoi cette formule porte ce nom, lorsque l'on remplace l'espérance par sa définition, dans les expressions précédentes :
        \begin{displaymath}
            \int_\Omega (g \circ X)(\omega) \text{d} \mathbb{P}(\omega) = \int_E g(x) \text{d} \mathbb{P}_X(x)
        \end{displaymath}
        Cette formule transfère le calcul sur un autre espace, muni d'une autre mesure.
        Pour peu que la mesure $\mathbb{P}_X$ soit facile à utiliser, on a intérêt à utiliser cette nouvelle formulation.
        % faire ref au changement de mesure du chap de la construction de l'intégrale.
    \end{rem}

    Voici quelques applications de ce théorème :

    \begin{exemple}
        Soit $X:\Omega \longrightarrow \R$ une variable aléatoire réelle.
        La fonction identité $\text{Id}:\R \longrightarrow \R$ est continue donc borélienne (voir la proposition~\ref{prop:continue_implique_borelienne}).

        En appliquant le théorème précédent, $X$ admet une espérance ssi $\text{Id}$ est $\mathbb{P}_X$-intégrable.
        Dans ce cas, on aura :
        \begin{displaymath}
            \esp(X) = \esp(\text{Id} \circ X) = \int_\R x\textnormal{d}\mathbb{P}_X(x)
        \end{displaymath}

        Pour peu que l'on sache expliciter $\mathbb{P}_X$, cette nouvelle l'expression de l'espérance nous simplifie grandement le calcul.

        Il est évident que l'on obtient le même résultat avec une variable aléatoire discrète.
    \end{exemple}

    \begin{exemple}
        De même, en choisissant la fonction carrée pour $g$ on obtient (si l'espérance de $X^2$ existe) :
        \begin{displaymath}
            \esp(X^2) = \int_\R x^2\textnormal{d}\mathbb{P}_X(x)
        \end{displaymath}

        Notons qu'en choisissant l'identité pour $g$, mais la variable aléatoire $X^2$, on obtient (à nouveau sous réserve d'existence) :
        \begin{displaymath}
            \esp(X^2) = \int_\mathbb{R} x\textnormal{d}\mathbb{P}_{X^2}(x)
        \end{displaymath}
    \end{exemple}

    \begin{rem}
        Voici deux cas particuliers pour la formule de transfert (cf.\ remarque~\ref{rem:esperances_particulieres}):
        \begin{itemize}
            \itemb si $X$ est une variable aléatoire réelle de densité $f$, alors la formule de transfert (lorsqu'elle s'applique) donne l'égalité suivante :
            \begin{displaymath}
                \esp(g \circ X) = \int_E g(x)f(x)\textnormal{d}x
            \end{displaymath}
            \itemb si $X$ est une variable aléatoire discrète, alors la formule de transfert (lorsqu'elle s'applique) donne l'égalité suivante (donnée ici dans le cas de $\N$, mais qui s'énonce de même dans les autres cas):
            \begin{displaymath}
                \esp(g \circ X) = \sum_{k=0}^{+\infty} g(k) \Pb(X = k)
            \end{displaymath}
        \end{itemize}
    \end{rem}

    \section{Moments d'un vecteur aléatoire}





\end{document}